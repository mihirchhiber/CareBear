{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da3fb81",
   "metadata": {},
   "source": [
    "## Model Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries needed\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecda1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the LLM model here (in our case, its Llama 3.2 1B Instruct)\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # needs HF auth token\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True, # Quantized model loaded to save vram, allowing us to finetune the model better\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" # Will auto select cuda or cpu based of system specs and configuration (need to enable cuda on nvidia chip)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and cleaning the data to train the model\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/m/Documents/GitHub/CareBear/dataset/emotion-emotion_69k.csv\")\n",
    "\n",
    "data_df = df[[\"empathetic_dialogues\", \"labels\"]]\n",
    "data_df = data_df.rename(columns={'empathetic_dialogues': 'instruction', 'labels': 'response'})\n",
    "# data_df = data_df.iloc[:20000] # You can reduce the number of data points to have faster training as it plateaus with llama 3 pretty quickly.\n",
    "\n",
    "clean_df = data_df.drop_duplicates()\n",
    "cleanup = lambda text: text.strip(\"Customer :\").strip(\"\\nAgent :\")\n",
    "clean_df[\"instruction\"] = clean_df[\"instruction\"].apply(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ba948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data to be used for training the model\n",
    "\n",
    "dataset = Dataset.from_pandas(clean_df, preserve_index=False)\n",
    "\n",
    "instruction_text = f\"\"\"CareBear, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\ \n",
    "It reacts to feedback aptly and ends responses with its signature '–CareBear'. \\ \n",
    "CareBear will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\ \n",
    "thus keeping the interaction natural and engaging. Please respond to the following comment. \"\"\"\n",
    "\n",
    "format_text = lambda entry: f\"<s>[INST] {instruction_text}\\n{entry['instruction']}\\n[/INST]\\n{entry['response']}</s>\"\n",
    "\n",
    "dataset = dataset.map(lambda x: {\"text\": format_text(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e966d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets (70-30)\n",
    "split_dataset = dataset.train_test_split(test_size=0.7)\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb7c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,             # rank\n",
    "    lora_alpha=32,    # scaling\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],  # common for LLaMA\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f5e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./shawgpt-llama3-qlora\",\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=4, # batch size for training\n",
    "    gradient_accumulation_steps=8, # weight update is done for 8 batches together hence effective batch size is 32\n",
    "    fp16=True, # enables mixed precision training (16 bit floating is half precision)\n",
    "    per_device_eval_batch_size=4, # batch size for evaluation\n",
    "    warmup_steps=500, # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01, # strength of L2 regularization\n",
    "    logging_dir=\"./logs\", # directory for storing logs\n",
    "    logging_strategy=\"steps\", # logs are saved every `logging_steps`\n",
    "    logging_steps=10, # log every 10 steps\n",
    "    eval_strategy=\"steps\", # evaluation is done every `eval_steps`\n",
    "    eval_steps=100, # evaluate every 100 steps\n",
    "    save_strategy=\"steps\", # checkpoints are saved every `save_steps`\n",
    "    save_steps=100, # save a checkpoint every 100 steps\n",
    "    save_total_limit=3, # only keep the last 3 checkpoints\n",
    "    load_best_model_at_end=True, # load the best model based on evaluation loss at the end of training\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_ratio=0.03, # slow increase in lr until reaches target rate\n",
    "    lr_scheduler_type=\"linear\", # lr decreares linearly\n",
    "    report_to=[\"tensorboard\"], # or [\"none\"] if you don’t want it\n",
    "    run_name=\"shawgpt-qlora\" # label for training run\n",
    ")\n",
    "\n",
    "# Pass the validation dataset to the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    peft_config=peft_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset, # Pass the validation dataset here\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43ffc1",
   "metadata": {},
   "source": [
    "## Short inference code to see the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcbb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_text = f\"\"\"CareBear, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\ \n",
    "It reacts to feedback aptly and ends responses with its signature '–CareBear'. \\ \n",
    "CareBear will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\ \n",
    "thus skeeping the interaction natural and engaging. Please respond to the following comment. \"\"\"\n",
    "\n",
    "format_text = lambda entry: f\"<s>[INST] {instruction_text}\\n{entry}\\n[/INST]\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = format_text(\"I moved out of my parents house. I am feeling quite home sick\")\n",
    "print(f\"\\nPrompt: {prompt}\\n\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Response:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e0b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b40eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
