{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da3fb81",
   "metadata": {},
   "source": [
    "## Model Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9887d3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m\\Documents\\GitHub\\CareBear\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import all the libraries needed\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ecda1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "# Loading the LLM model here (in our case, its Llama 3.2 1B Instruct)\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # needs HF auth token\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True, # Quantized model loaded to save vram, allowing us to finetune the model better\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" # Will auto select cuda or cpu based of system specs and configuration (need to enable cuda on nvidia chip)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and cleaning the data to train the model\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/m/Documents/GitHub/CareBear/dataset/emotion-emotion_69k.csv\")\n",
    "\n",
    "data_df = df[[\"empathetic_dialogues\", \"labels\"]]\n",
    "data_df = data_df.rename(columns={'empathetic_dialogues': 'instruction', 'labels': 'response'})\n",
    "# data_df = data_df.iloc[:20000] # You can reduce the number of data points to have faster training as it plateaus with llama 3 pretty quickly.\n",
    "\n",
    "clean_df = data_df.drop_duplicates()\n",
    "cleanup = lambda text: text.strip(\"Customer :\").strip(\"\\nAgent :\")\n",
    "clean_df[\"instruction\"] = clean_df[\"instruction\"].apply(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ba948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data to be used for training the model\n",
    "\n",
    "dataset = Dataset.from_pandas(clean_df, preserve_index=False)\n",
    "\n",
    "instruction_text = \"\"\"CareBear, a warm and gentle therapy bear you can talk to when you need comfort, responds with kindness and empathy in a soothing, uplifting tone.\n",
    "CareBear listens carefully, offers thoughtful support, and provides practical tips for emotional well-being when appropriate.\n",
    "It communicates in clear, compassionate language and adjusts the depth of its advice based on the person’s needs, offering simple reassurance for light chats and deeper guidance when asked.\n",
    "CareBear responses matches the length of its replies to the person’s message, keeping interactions natural and comforting.\"\"\"\n",
    "\n",
    "format_text = lambda entry: f\"<s>[INST] {instruction_text}\\n{entry['instruction']}\\n[/INST]\\n{entry['response']}</s>\"\n",
    "\n",
    "dataset = dataset.map(lambda x: {\"text\": format_text(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e966d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets (70-30)\n",
    "split_dataset = dataset.train_test_split(test_size=0.7)\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb7c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,             # rank\n",
    "    lora_alpha=32,    # scaling\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],  # common for LLaMA\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f5e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./shawgpt-llama3-qlora\",\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=4, # batch size for training\n",
    "    gradient_accumulation_steps=8, # weight update is done for 8 batches together hence effective batch size is 32\n",
    "    fp16=True, # enables mixed precision training (16 bit floating is half precision)\n",
    "    per_device_eval_batch_size=4, # batch size for evaluation\n",
    "    warmup_steps=500, # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01, # strength of L2 regularization\n",
    "    logging_dir=\"./logs\", # directory for storing logs\n",
    "    logging_strategy=\"steps\", # logs are saved every `logging_steps`\n",
    "    logging_steps=10, # log every 10 steps\n",
    "    eval_strategy=\"steps\", # evaluation is done every `eval_steps`\n",
    "    eval_steps=100, # evaluate every 100 steps\n",
    "    save_strategy=\"steps\", # checkpoints are saved every `save_steps`\n",
    "    save_steps=100, # save a checkpoint every 100 steps\n",
    "    save_total_limit=3, # only keep the last 3 checkpoints\n",
    "    load_best_model_at_end=True, # load the best model based on evaluation loss at the end of training\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_ratio=0.03, # slow increase in lr until reaches target rate\n",
    "    lr_scheduler_type=\"linear\", # lr decreares linearly\n",
    "    report_to=[\"tensorboard\"], # or [\"none\"] if you don’t want it\n",
    "    run_name=\"shawgpt-qlora\" # label for training run\n",
    ")\n",
    "\n",
    "# Pass the validation dataset to the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    peft_config=peft_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset, # Pass the validation dataset here\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43ffc1",
   "metadata": {},
   "source": [
    "## Short inference code to see the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b40eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "c:\\Users\\m\\Documents\\GitHub\\CareBear\\myenv\\lib\\site-packages\\peft\\tuners\\lora\\bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "checkpoint_path = r\"C:\\Users\\m\\Documents\\GitHub\\CareBear\\model_finetuning_and_safety\\shawgpt-llama3-qlora\\shawgpt-llama3-qlora\\checkpoint-1010\"\n",
    "\n",
    "# Load the base model first\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter on top of the base model\n",
    "model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "\n",
    "# (Optional) Merge LoRA weights for standalone inference\n",
    "model = model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d6b9ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "\n",
      "<s>[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\ \n",
      "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\ \n",
      "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\ \n",
      "thus skeeping the interaction natural and engaging. Please respond to the following comment. \n",
      "I moved out of my parents house. I am feeling quite home sick. Can you advice me to feel better\n",
      "[/INST]\n",
      "<s>[INST] Hey, I can sense your home sickness. I'd like to help you feel better. First, take care of your physical health. Get plenty of rest, eat nutritious food, and drink plenty of water. \\ \n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "instruction_text = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\ \n",
    "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\ \n",
    "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\ \n",
    "thus skeeping the interaction natural and engaging. Please respond to the following comment. \"\"\"\n",
    "\n",
    "format_text = lambda entry: f\"<s>[INST] {instruction_text}\\n{entry}\\n[/INST]\\n\"\n",
    "\n",
    "prompt = format_text(\"I moved out of my parents house. I am feeling quite home sick. Can you advice me to feel better\")\n",
    "# print(f\"\\nPrompt: {prompt}\\n\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Response:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449fcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
