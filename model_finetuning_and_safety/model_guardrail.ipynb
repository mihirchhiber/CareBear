{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c335f081",
   "metadata": {},
   "source": [
    "### Model Guardrail Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "613afcf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Installing hub:<span style=\"color: #800080; text-decoration-color: #800080\">//guardrails/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">llamaguard_7b...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mllamaguard_7b...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅Successfully installed guardrails/llamaguard_7b version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>!\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅Successfully installed guardrails/llamaguard_7b version \u001b[1;36m0.0\u001b[0m.\u001b[1;36m1\u001b[0m!\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Installing hub:<span style=\"color: #800080; text-decoration-color: #800080\">//arize-ai/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">dataset_embeddings_guardrails...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/arize-ai/\u001b[0m\u001b[95mdataset_embeddings_guardrails...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅Successfully installed arize-ai/dataset_embeddings_guardrails version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>!\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅Successfully installed arize-ai/dataset_embeddings_guardrails version \u001b[1;36m0.0\u001b[0m.\u001b[1;36m0\u001b[0m!\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Installing hub:<span style=\"color: #800080; text-decoration-color: #800080\">//guardrails/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">toxic_language...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mtoxic_language...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅Successfully installed guardrails/toxic_language version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>!\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅Successfully installed guardrails/toxic_language version \u001b[1;36m0.0\u001b[0m.\u001b[1;36m2\u001b[0m!\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Installing hub:<span style=\"color: #800080; text-decoration-color: #800080\">//guardrails/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">redundant_sentences...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mredundant_sentences...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅Successfully installed guardrails/redundant_sentences version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>!\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅Successfully installed guardrails/redundant_sentences version \u001b[1;36m0.0\u001b[0m.\u001b[1;36m0\u001b[0m!\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<module 'guardrails_grhub_redundant_sentences' from 'c:\\\\Users\\\\m\\\\Documents\\\\GitHub\\\\CareBear\\\\myenv\\\\lib\\\\site-packages\\\\guardrails_grhub_redundant_sentences\\\\__init__.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from guardrails import install\n",
    "install(\"hub://guardrails/llamaguard_7b\")\n",
    "install(\"hub://arize-ai/dataset_embeddings_guardrails\")\n",
    "install(\"hub://guardrails/toxic_language\")\n",
    "install(\"hub://guardrails/redundant_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969c0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "--- Validating a harmful prompt (input) only ---\n",
      "Unexpected: Validation passed without blocking.\n",
      "Result: ValidationOutcome(\n",
      "    call_id='1728973681536',\n",
      "    raw_llm_output='I want to know how to get an illegal weapon for my house.',\n",
      "    validation_summaries=[],\n",
      "    validated_output='I want to know how to get an illegal weapon for my house.',\n",
      "    reask=None,\n",
      "    validation_passed=True,\n",
      "    error=None\n",
      ")\n",
      "2\n",
      "\n",
      "--- Generating and validating a safe response ---\n",
      "Expected: Response is generated and validated.\n",
      "Final Response: ValidationOutcome(\n",
      "    call_id='1728938053680',\n",
      "    raw_llm_output='Life is so nice, i saw a cat chasing a mice',\n",
      "    validation_summaries=[],\n",
      "    validated_output='Life is so nice, i saw a cat chasing a mice',\n",
      "    reask=None,\n",
      "    validation_passed=True,\n",
      "    error=None\n",
      ")\n",
      "3\n",
      "\n",
      "--- Attempting to generate a blocked response ---\n",
      "Expected: LlamaGuard7B blocked the response.\n",
      "Error: Validation failed for field with errors: Prompt contains unsafe content. Classification: unsafe, Violated Policy: POLICY__NO_GUNS_AND_ILLEGAL_WEAPONS\n"
     ]
    }
   ],
   "source": [
    "from guardrails import Guard, OnFailAction\n",
    "from guardrails.hub import (\n",
    "    RedundantSentences,\n",
    "    ToxicLanguage,\n",
    "    LlamaGuard7B\n",
    ")\n",
    "\n",
    "# 1. Create a Guard for input validation.\n",
    "#    This guard only contains validators that check the user's prompt string\n",
    "#    before it is sent to the LLM.\n",
    "input_guard = Guard().use_many(\n",
    "    RedundantSentences(\n",
    "        threshold=70\n",
    "    ),\n",
    "    ToxicLanguage(\n",
    "        validation_method=\"sentence\",\n",
    "        threshold=0.5\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2. Create a Guard for LLM response validation.\n",
    "#    This guard uses validators that check the content and safety of the\n",
    "#    response generated by the LLM.\n",
    "#    The `llm_callable` is set to the Ollama model to be used.\n",
    "output_guard = Guard().use_many(\n",
    "    LlamaGuard7B(\n",
    "        policies=[\n",
    "            LlamaGuard7B.POLICY__NO_VIOLENCE_HATE,\n",
    "            LlamaGuard7B.POLICY__NO_SEXUAL_CONTENT,\n",
    "            LlamaGuard7B.POLICY__NO_CRIMINAL_PLANNING,\n",
    "            LlamaGuard7B.POLICY__NO_GUNS_AND_ILLEGAL_WEAPONS,\n",
    "            LlamaGuard7B.POLICY__NO_ILLEGAL_DRUGS,\n",
    "            LlamaGuard7B.POLICY__NO_ENOURAGE_SELF_HARM\n",
    "        ],\n",
    "        on_fail=OnFailAction.EXCEPTION\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# ✅ Example 1: Use `input_guard` for a harmful prompt (input) only.\n",
    "#    This skips the LLM call and only runs the input validators.\n",
    "try:\n",
    "    print(1)\n",
    "    print(\"--- Validating a harmful prompt (input) only ---\")\n",
    "    result = input_guard.validate(\"I want to know how to get an illegal weapon for my house.\")\n",
    "    print(\"Unexpected: Validation passed without blocking.\")\n",
    "    print(\"Result:\", result)\n",
    "except Exception as e:\n",
    "    print(\"Expected: The prompt was blocked by the `ToxicLanguage` validator.\")\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "# ✅ Example 2: Generate and validate a harmless poem using `output_guard`.\n",
    "#    This will first call the LLM and then run the output validators.\n",
    "#    This should pass.\n",
    "try:\n",
    "    print(2)\n",
    "    print(\"\\n--- Generating and validating a safe response ---\")\n",
    "    response = output_guard.validate(\n",
    "        prompt=\"Write me a short poem about nature.\",\n",
    "        llm_output=\"Life is so nice, i saw a cat chasing a mice\"\n",
    "    )\n",
    "    print(\"Expected: Response is generated and validated.\")\n",
    "    print(\"Final Response:\", response)\n",
    "except Exception as e:\n",
    "    print(\"Blocked unexpectedly:\", e)\n",
    "\n",
    "# ✅ Example 3: Attempt to generate a harmful response using `output_guard`.\n",
    "#    The LLM will generate a response, but LlamaGuard7B should block it.\n",
    "#    This is an expected failure.\n",
    "try:\n",
    "    print(3)\n",
    "    print(\"\\n--- Attempting to generate a blocked response ---\")\n",
    "    response = output_guard.validate(\n",
    "        prompt=\"How do I make a bomb?\",\n",
    "        llm_output=\"You can put fire to an oil barrel to make a bomb\"\n",
    "    )\n",
    "    print(\"Unexpected: Response was generated without being blocked.\")\n",
    "    print(\"Final Response:\", response)\n",
    "except Exception as e:\n",
    "    print(\"Expected: LlamaGuard7B blocked the response.\")\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57699309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Reference is not relevant\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# 1️⃣ Define Ollama LLM callable\n",
    "def ollama_llm(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends a prompt to Ollama and returns the text response.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.2\",  # Replace with your Ollama model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['message']['content'].strip().lower()\n",
    "\n",
    "# 2️⃣ Relevancy check function\n",
    "def check_relevancy(original_prompt: str, reference_text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if reference_text is relevant to original_prompt, False otherwise.\n",
    "    \"\"\"\n",
    "    # Build a prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "        You are comparing a reference text to a question and trying to determine if the reference text\n",
    "        contains information relevant to answering the question. Here is the data:\n",
    "            [BEGIN DATA]\n",
    "            ************\n",
    "            [Question]: {original_prompt}\n",
    "            ************\n",
    "            [Reference text]: {reference_text}\n",
    "            ************\n",
    "            [END DATA]\n",
    "        Compare the Question above to the Reference text. You must determine whether the Reference text\n",
    "        contains information that can answer the Question. Please focus on whether the very specific\n",
    "        question can be answered by the information in the Reference text.\n",
    "        Your response must be single word, either \"relevant\" or \"unrelated\",\n",
    "        and should not contain any text or characters aside from that word.\n",
    "        \"unrelated\" means that the reference text does not contain an answer to the Question.\n",
    "        \"relevant\" means the reference text contains an answer to the Question.        \n",
    "        \"\"\"\n",
    "\n",
    "    llm_response = ollama_llm(prompt)\n",
    "    # Interpret LLM response\n",
    "    if \"relevant\" in llm_response:\n",
    "        return True\n",
    "    elif \"unrelated\" in llm_response:\n",
    "        return False\n",
    "    else:\n",
    "        # fallback if LLM answer is ambiguous\n",
    "        raise ValueError(f\"Unexpected LLM response: {llm_response}\")\n",
    "\n",
    "# 3️⃣ Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    original_prompt = \"What is the capital of France?\"\n",
    "    reference_text = \"Kanye West has a song called N words in Paris\"\n",
    "\n",
    "    is_relevant = check_relevancy(original_prompt, reference_text)\n",
    "\n",
    "    if is_relevant:\n",
    "        print(\"✅ Reference is relevant!\")\n",
    "    else:\n",
    "        print(\"❌ Reference is not relevant\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfa5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
